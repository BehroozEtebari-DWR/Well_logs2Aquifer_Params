{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6201d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4eb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with 'latin1' encoding\n",
    "try:\n",
    "    df_svsim = pd.read_csv(\"svsim_texture_data.csv\", encoding='latin1')\n",
    "except UnicodeDecodeError:\n",
    "    # If 'latin1' fails, try with 'cp1252' encoding\n",
    "    df_svsim = pd.read_csv(\"svsim_texture_data.csv\", encoding='cp1252')\n",
    "\n",
    "# Assuming df4 is the DataFrame where you want to rename the column\n",
    "df_svsim.rename(columns={'X': 'UTMX','Y': 'UTMY', 'TOP_BGS':'INTERVALSTART',\n",
    "              'BASE_BGS':'INTERVALEND', 'LITH_DESC': 'DESCRIPTION' }, inplace=True)\n",
    "\n",
    "# Display the first few rows to confirm the renaming\n",
    "df_svsim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7356f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the 'DESCRIPTION' column exists\n",
    "if 'DESCRIPTION' not in df_svsim.columns:\n",
    "    raise KeyError(\"The 'DESCRIPTION' column does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6be014-8b47-4b25-bf66-ed07ca8cc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace typos in the 'DESCRIPTION' column of the DataFrame\n",
    "df_svsim['DESCRIPTION'] = df_svsim['DESCRIPTION'].replace({\n",
    "    'mudstrone': 'mudstone',\n",
    "    'sandsttone': 'sandstone',\n",
    "    'Hard Pan': 'hardpan'\n",
    "}, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dc9f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the USCS codes and known USCS names globally\n",
    "uscs_keywords_keep = [\n",
    "    '(ML)', '(GP)', '(SP-SM)', '(GC)', '(CL)', '(CH)', '(MH)', '(GP-GM)', '(CL/ML)', '(GW/GM)',\n",
    "    '(GP-GC)', '(ML-SM)', '(SP-SM)', '(SC-SM)', '(SC)', '(SW/GW)', '(SM/GM)', '(GP-SP)',\n",
    "    '(CL/SM)', '(ML/GW)', '(SP & GP)', '(GW/GC)', '(GW-GM/GW)', '(SP-SC)','(SP/SM)', '(SM/SW)','(SP/GP/COBL)',\n",
    "]\n",
    "\n",
    "# Function to extract USCS codes from text within parentheses and direct codes\n",
    "def extract_uscs(text):\n",
    "    if isinstance(text, str):\n",
    "        text_upper = text.upper()  # Convert text to uppercase for case insensitivity\n",
    "\n",
    "        # Check for USCS code within parentheses\n",
    "        match = re.search(r'\\((.*?)\\)', text_upper)\n",
    "        if match:\n",
    "            code_in_parentheses = f\"({match.group(1).strip()})\"\n",
    "            if code_in_parentheses in uscs_codes:\n",
    "                cleaned_text = text[:match.start()].strip() + \" \" + text[match.end():].strip()\n",
    "                return code_in_parentheses, cleaned_text.strip()\n",
    "\n",
    "        # Check if the text itself is a valid USCS code\n",
    "        if text_upper in uscs_keywords_keep:\n",
    "            return text_upper, text\n",
    "\n",
    "    return 'unknown', text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322bcfc-a530-440e-aaa9-4d661bfa3490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to convert inaccurate USCS \n",
    "USCS_conversions = {\n",
    "    'SHLE': [ 'shale'],\n",
    "    'CLSN':['claystone','mudstone'],\n",
    "    'STST':['slst','siltstone'],\n",
    "    'TPSL':['topsoil', 'soil/organic','tp'],\n",
    "    'VFRG':['volcanic frags'],\n",
    "    'CONG':['conglomerate'],\n",
    "    'COBL':['cobbles'],\n",
    "    'SDST':['sandstone'],\n",
    "    'TUFF':['tuff'],\n",
    "    'ASH':['ash'],\n",
    "    'GW/SW':['other-fine'],\n",
    "    'CH/ML':['other-coarse'],\n",
    "    'FRAC':['FRCT'],\n",
    "}\n",
    "\n",
    "# Define a function to apply USCS conversions (case-insensitive)\n",
    "def convert_uscs(uscs_value, conversions):\n",
    "    # Ensure the value is a string\n",
    "    if isinstance(uscs_value, str):  \n",
    "        uscs_value_lower = uscs_value.lower()  # Convert the USCS value to lowercase\n",
    "        for key, values in conversions.items():\n",
    "            # Check if the USCS value matches any of the dictionary values (case-insensitive)\n",
    "            if uscs_value_lower in [v.lower() for v in values]:\n",
    "                return key  # Return the correct USCS classification\n",
    "    return uscs_value  # If no match, return the original value\n",
    "\n",
    "# Apply the conversion function to the 'USCS' column in the df_svsim DataFrame\n",
    "df_svsim['USCS'] = df_svsim['USCS'].apply(lambda x: convert_uscs(x, USCS_conversions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define qualifiers\n",
    "color_qualifiers = ['red', 'green', 'black', 'brown', 'gray', 'grayish', 'white','greenish',\n",
    "                    'reddish', 'yellow', 'dark', 'light', 'tan', 'colored', 'blue','brownblack',\n",
    "                   'yellowish','purple', 'orange','brw.','grey','(blue)', 'red,'  ]\n",
    "\n",
    "texture_qualifiers = ['loose', 'hard', 'coarse', 'fine', 'compacted', 'cemented','crushed',\n",
    "           'salt & pepper','Minor','medium', 'large','firm', 'small', 'fracture', 'frac','little',\n",
    "           'fractured', 'soft', 'minor', 'eroded','tight','broken', 'brittle','chunky', 'crusty',\n",
    "           'med.','packed', 'brittle', 'porous', 'pea', 'welded', 'mixed', 'softer', 'joint',\n",
    "           'chunky','large', 'big','solid','firm','hard','heavy','very stiff', '(solid)', '(cement)',\n",
    "           'laminated','poorly graded', 'well graded', 'sticky', 'grained','graded', 'med','dry' ,   \n",
    "            '(cemented)', '(set)', '(water)','soft-med' ,'stiff' ,'crumbly' , 'granulated' ,'streaky',       \n",
    "            'tough' ,'(varied)' ,  'gritty','holey', 'impervious' ,  'rubbery', 'rough','stringers',      \n",
    "             'ashy', 'porous', '(balls)' ,'(tough)', '(hard drilling)','mottled', 'poorly',\n",
    "             'no cementation', 'is grained', 'subrounded', 'no staining', 'no odor','with holes'       \n",
    "              , '( water)' , 'water']\n",
    "\n",
    "# Copy data from 'DESCRIPTION' to a new column 'TEXTURE'\n",
    "df_svsim['TEXTURE'] = df_svsim['DESCRIPTION']\n",
    "\n",
    "# Function to extract qualifiers from a string\n",
    "def extract_qualifiers(description, qualifiers):\n",
    "    if pd.isna(description):\n",
    "        return []\n",
    "    words = description.lower().split()\n",
    "    return [word for word in words if word in qualifiers]\n",
    "\n",
    "# Extract COLORQUALIFIER and TEXTUREQUALIFIER\n",
    "df_svsim['COLORQUALIFIER_EXTRACTED'] = df_svsim.apply(lambda row: extract_qualifiers(row['TEXTURE'], color_qualifiers), axis=1)\n",
    "df_svsim['TEXTUREQUALIFIER_EXTRACTED'] = df_svsim.apply(lambda row: extract_qualifiers(row['TEXTURE'], texture_qualifiers), axis=1)\n",
    "\n",
    "# Convert lists to strings\n",
    "df_svsim['COLORQUALIFIER_EXTRACTED'] = df_svsim['COLORQUALIFIER_EXTRACTED'].apply(lambda x: ' '.join(x) if x else np.nan)\n",
    "df_svsim['TEXTUREQUALIFIER_EXTRACTED'] = df_svsim['TEXTUREQUALIFIER_EXTRACTED'].apply(lambda x: ' '.join(x) if x else np.nan)\n",
    "\n",
    "# Function to remove qualifiers from a string\n",
    "def remove_qualifiers(description, qualifiers):\n",
    "    if pd.isna(description):\n",
    "        return description\n",
    "    words = description.lower().split()\n",
    "    return ' '.join([word for word in words if word not in qualifiers])\n",
    "\n",
    "# Create DESCRIPTION2 column\n",
    "df_svsim['NEW_DESCRIPTION'] = df_svsim.apply(lambda row: remove_qualifiers(row['TEXTURE'], color_qualifiers + texture_qualifiers), axis=1)\n",
    "\n",
    "# Handle missing 'TEXTUREMODIFIER1' column\n",
    "if 'TEXTUREMODIFIER1' not in df_svsim.columns:\n",
    "    df_svsim['TEXTUREMODIFIER1'] = np.nan\n",
    "\n",
    "# Concatenate TEXTUREMODIFIER1 and DESCRIPTION if they are not equal, and include new columns\n",
    "df_svsim['TEXTURE_MODIFIED'] = df_svsim.apply(\n",
    "    lambda row: (row['TEXTUREMODIFIER1'] + ' ' if pd.notna(row['TEXTUREMODIFIER1']) and row['TEXTUREMODIFIER1'] != row['TEXTURE'] else '') + (row['DESCRIPTION'] if pd.notna(row['DESCRIPTION']) else ''),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Update original COLORQUALIFIER and TEXTUREQUALIFIER columns\n",
    "df_svsim['COLORQUALIFIER'] = df_svsim['COLORQUALIFIER_EXTRACTED']\n",
    "df_svsim['TEXTUREQUALIFIER'] = df_svsim['TEXTUREQUALIFIER_EXTRACTED']\n",
    "\n",
    "# Drop the intermediate columns if needed\n",
    "df_svsim.drop(columns=['COLORQUALIFIER_EXTRACTED', 'TEXTUREQUALIFIER_EXTRACTED','TEXTURE','TEXTUREMODIFIER1','TEXTURE_MODIFIED',],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26193f-bb5c-4745-afc6-93e00751ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_keep = {\n",
    "    # Compound soil types\n",
    "    'gravelly clay', 'sandy loam', 'silty clay', 'pebbly loam', 'cobbly sand', 'sandy mud',\n",
    "    'clayey loam', 'sandy clay', 'silty loam', 'gravelly sand', 'pebbley sand','clayey wood',\n",
    "    'cobbley clay', 'loamy sand', 'clayey gravel', 'gravelly loam', 'pebbley clay','gravelly wood',\n",
    "    'sandy gravel', 'clayey sand', 'silty gravel', 'loamy gravel', 'silty sand','mucky mud',\n",
    "    'gravelly silt', 'pebbley gravel', 'cobbley loam', 'clayey silt', 'gravelly clayey sand',\n",
    "    'loamy clay', 'pebbley loamy', 'sandy silty', 'cobbley gravel', 'clayey sandy','cobbley sandy pebbles',\n",
    "    'silty cobbly', 'gravelly pebbly', 'sandy cobble', 'gravelly sandy','gravelly cobbles','cobbley sandy pebbles',\n",
    "    'rocky clay', 'rocky loam', 'rocky gravel', 'rocky silt','rocky sand','gravelly boulders','cobbley gravelly wood',\n",
    "    'cobbley gravelly pebbles',\n",
    "    # Sedimentary rocks\n",
    "    'sandstone', 'conglomerate', 'shale', 'siltstone', 'limestone', 'cobblestone',  'mudstone', \n",
    "    \n",
    "    # Soil classifications\n",
    "    'silt', 'sand', 'gravel',  'clay', 'boulder', 'loam', 'cobble', 'gravels','cobbles','boulders','clays','mud', 'wood',\n",
    "    'pebbles',\n",
    "\n",
    "    # Soil descriptors\n",
    "    'sticky clay', 'fat clay', 'lean clay', 'hardpan','pan', 'organic', 'adobe',  'weathered','poorly graded sand', 'well graded sand',\n",
    "    'poorly graded gravel', 'well graded gravel',\n",
    "    # Rocks and minerals\n",
    "    'basalt', 'basaltic', 'pumice', 'latite', 'volcanics', 'volcanic', 'cinder', 'ash', 'lime',\n",
    "    'tufa', 'tuff', 'lava', 'rhyolite', 'granite', 'diorite', 'quartz', 'gabbro', 'quartzite', \n",
    "    'granodiorite', 'igneous', 'andesite', 'greenstone', 'slate', 'schist', 'serpentine', \n",
    "    'metasediment', 'phyllite', 'argillite', 'bluestone', 'soapstone', 'chert','fractured',\n",
    "    'fractured rock', 'gouge', 'hardrock', 'rock','bedrock','frac','tuscan','lapilli','limestone',\n",
    "\n",
    "    # Other geological terms\n",
    "    'sediment', 'alluvium', 'loam', 'peat', 'organics','topsoil','soil', 'organic'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16413b19-2340-49d8-b6ec-dacd580406b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the key words to convert into USCS\n",
    "descrip = df_svsim['DESCRIPTION']\n",
    "\n",
    "# Build the regex pattern\n",
    "pattern = r'\\b(?:' + '|'.join(map(re.escape, keywords_keep)) + r')\\b'\n",
    "\n",
    "# Apply the regex, handling NaN values by converting them to an empty string\n",
    "extracted_words = [re.findall(pattern, str(d), re.IGNORECASE) for d in descrip]\n",
    "\n",
    "# Add the extracted words as a new column\n",
    "df_svsim['KEYWORDS_LIST'] = extracted_words\n",
    "\n",
    "# Join the keywords into a single string\n",
    "new_descriptions = df_svsim['KEYWORDS_LIST']\n",
    "separator = ' ,'\n",
    "d = []\n",
    "\n",
    "for new_description in new_descriptions:\n",
    "    new_descrip = separator.join(new_description)\n",
    "    d.append(new_descrip.lower())\n",
    "\n",
    "# Add the final keywords column\n",
    "df_svsim['KEYWORDS'] = d\n",
    "\n",
    "# Drop the intermediate column\n",
    "df_svsim = df_svsim.drop(['KEYWORDS_LIST'], axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_svsim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ad452",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_mapping = {\n",
    "    # USCS Soil Classifications\n",
    "    'CH': ['clay','clays', 'adobe clay', 'sticky clay', 'fat clay', 'adobe'],\n",
    "    'CH-SC': ['clay sand'],\n",
    "    'SC-GP': ['gravelly clayey sand'],\n",
    "    'CL': ['lean clay', 'silty clay', 'sandy clay', 'gravelly clay',],\n",
    "    'OH': ['organic clay'], \n",
    "    'TPSL': ['soil', 'top soil', 'topsoil', 'silty soil'],\n",
    "    'GP': ['poorly graded gravel', 'gravel','cobbley gravelly pebbles','gravels'],\n",
    "    'COBL': ['cobble', 'boulder','cobbles', 'boulders', 'pebbles'],\n",
    "    'GW': ['well graded gravel', 'alluvium', 'pebbles', 'pebbley gravel', 'gravely cobbles', 'decomposed granite'],\n",
    "    'SP': ['poorly graded sand', 'sand'],\n",
    "    'SP-GP': ['gravelly sand', 'sandy gravel','sediment'],\n",
    "    'SW': ['well graded sand', 'sandy'],\n",
    "    'SW-GW': ['pebbley sand'],\n",
    "    'SM-SC': ['sandy shale and sand'],\n",
    "    'SM': ['silty sand'],\n",
    "    'SC': ['clayey sand'],\n",
    "    'ML': ['loam', 'clayey loam', 'hardpan','pan','silt', 'sandy silt', 'clayey silt', 'sandy shale'],\n",
    "    'PT': ['muck', 'peat', 'organics', 'mud', 'mucky mud', 'wood'],\n",
    "    'CL-PT': ['woody clay'],\n",
    "    'GC': ['gravelly clayey'],\n",
    "    'GM':['silty pebbles'],\n",
    "\n",
    "    # Volcanic and Igneous Rocks\n",
    "    'BSLT': ['basalt','andesite','latite', 'basaltic',],\n",
    "    'VOLC': [ 'volcanics','volcanic'],\n",
    "    'ASH': ['ash'],\n",
    "    'LAVA': ['lava' ],\n",
    "    'TUFF': ['tuff'],\n",
    "    'VFRG': ['pumice'],    \n",
    "    'IGNS': ['diorite', 'gabbro' ],\n",
    "    'GRNT': ['granite', 'quartzite', 'granodiorite','quartz'],\n",
    "    \n",
    "    # Metamorphic Rocks\n",
    "    'SCHT': ['slate', 'schist'],\n",
    "    'META': ['greenstone',  'serpentine', 'phyllite', 'argillite', 'soapstone'],\n",
    "\n",
    "    # Sedimentary Rocks\n",
    "    'SDST': ['sandstone'],\n",
    "    'CONG': ['conglomerate', 'cobblestone'],\n",
    "    'SHLE': ['shale'], \n",
    "    'STST': ['siltstone'],\n",
    "    'CLSN': ['mudstone'],\n",
    "    'LMST': ['limestone','lime'],\n",
    "    'LMST-CL': ['clayey lime'],\n",
    "\n",
    "    # Double Porosity Rocks (fractured rocks)\n",
    "    'FRAC': ['fractured', 'fracture', 'fractured rock'],\n",
    "\n",
    "    # Miscellaneous Classifications\n",
    "    'GP-OH': ['dirty gravel'],\n",
    "    'SP-OH': ['dirty sand'],\n",
    "    'OH': ['dirty top soil','mucky mud', 'organic'],\n",
    "    'CL-GRNT': ['granitic clay'],\n",
    "    'SP-GRNT': ['granitic sand'],\n",
    "    'ROCK': ['rock' , 'chert','bedrock',],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to extract USCS codes from text within parentheses and direct codes\n",
    "def extract_uscs(text, current_uscs):\n",
    "    # If there is already a value in the USCS column, return it\n",
    "    if isinstance(current_uscs, str) and current_uscs.lower() != 'unknown':\n",
    "        return current_uscs\n",
    "\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()  # Convert text to lower case\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\- ,%]', '', text)  # Allow hyphens, commas, and percentages\n",
    "\n",
    "        # Check if the cleaned text is empty\n",
    "        if not text.strip():\n",
    "            return 'unknown'  # If the cleaned text is empty, return 'unknown'\n",
    "\n",
    "        # Tokenize the text into individual words or phrases\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Track all matches\n",
    "        matched_soils = []\n",
    "\n",
    "        # Check for exact matches in the dictionary for each token\n",
    "        for token in tokens:\n",
    "            for key, synonyms in classification_mapping.items():\n",
    "                if token in synonyms:\n",
    "                    matched_soils.append(key.upper())\n",
    "\n",
    "        # If we have any exact matches, return them (multiple can be combined)\n",
    "        if matched_soils:\n",
    "            return ','.join(set(matched_soils))  # Remove duplicates and join\n",
    "\n",
    "        # If no exact match is found, try using rapidfuzz for approximate matching\n",
    "        for token in tokens:\n",
    "            best_match = process.extractOne(token, classification_mapping.keys(), scorer=fuzz.token_sort_ratio)\n",
    "            if best_match and best_match[1] > 75:  # Threshold can be adjusted\n",
    "                matched_soils.append(best_match[0].upper())\n",
    "\n",
    "        # Return the closest matches found using rapidfuzz, if any\n",
    "        if matched_soils:\n",
    "            return ','.join(set(matched_soils))  # Remove duplicates and join\n",
    "\n",
    "        # Check for USCS code within parentheses (if provided)\n",
    "        match = re.search(r'\\((.*?)\\)', text)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "\n",
    "    return 'unknown'\n",
    "\n",
    "# Applying the extract_uscs function to the DataFrame\n",
    "df_svsim['USCS'] = df_svsim.apply(lambda row: extract_uscs(row['KEYWORDS'], row['USCS']), axis=1)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_svsim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefe3d03-e5ab-45f1-ab1b-99a861392560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the provided Excel file\n",
    "file_path = r'C:\\Users\\betebari\\Documents\\C2VSim_Texture\\OSWCR\\USCS-averageKxy-CoarseFractions.xlsx'\n",
    "excel_data = pd.read_excel(file_path)\n",
    "\n",
    "# Strip any leading/trailing spaces from 'Sediment/Rock Type' column in the Excel data\n",
    "excel_data['Sediment/Rock Type'] = excel_data['Sediment/Rock Type'].str.strip()\n",
    "\n",
    "# Convert 'Sediment/Rock Type' to lowercase for case-insensitive matching\n",
    "excel_data['Sediment/Rock Type'] = excel_data['Sediment/Rock Type'].str.lower()\n",
    "\n",
    "# Create a dictionary mapping Soil Classification to Average Hydraulic Conductivity (case-insensitive)\n",
    "hydraulic_conductivity_mapping = dict(zip(excel_data['Sediment/Rock Type'], excel_data['Average Hydraulic Conductivity (ft/day)']))\n",
    "\n",
    "# Create a dictionary mapping Soil Classification to Average Coarse Fraction (case-insensitive)\n",
    "coarse_fraction_mapping = dict(zip(excel_data['Sediment/Rock Type'], excel_data['Average Coarse Fraction (%)']))\n",
    "\n",
    "# Convert 'USCS' column to lowercase for case-insensitive matching\n",
    "df_svsim['USCS'] = df_svsim['USCS'].str.lower()\n",
    "\n",
    "# Function to handle the slash (50/50) and dash (sequential rule) logic\n",
    "def aggregate_uscs_values(uscs_value, mapping, agg_func='average'):\n",
    "    # Split USCS string into parts by commas, slashes, and dashes\n",
    "    if '/' in uscs_value:\n",
    "        # For slash, treat as 50/50\n",
    "        uscs_list = [item.strip().lower() for item in uscs_value.split('/')]\n",
    "        values = [mapping.get(uscs) for uscs in uscs_list if uscs in mapping]\n",
    "        # Take the average for 50/50 mixtures\n",
    "        if values:\n",
    "            return sum(values) / len(values)\n",
    "    elif '-' in uscs_value:\n",
    "        # For dash, follow the first one that matches\n",
    "        uscs_list = [item.strip().lower() for item in uscs_value.split('-')]\n",
    "        for uscs in uscs_list:\n",
    "            if uscs in mapping:\n",
    "                return mapping.get(uscs)  # Return the first match\n",
    "    else:\n",
    "        # If no special characters, treat as a single or comma-separated list\n",
    "        uscs_list = [item.strip().lower() for item in uscs_value.split(',')]\n",
    "        values = [mapping.get(uscs) for uscs in uscs_list if uscs in mapping]\n",
    "\n",
    "    # Apply 12% coarse fraction if secondary USCS classification is present and no match found\n",
    "    if 'gc' in uscs_list or 'sc' in uscs_list or 'gm' in uscs_list or 'sm' in uscs_list:\n",
    "        return 12 if not values else sum(values) / len(values)\n",
    "\n",
    "    if values:\n",
    "        if agg_func == 'average':\n",
    "            return sum(values) / len(values)\n",
    "        elif agg_func == 'max':\n",
    "            return max(values)\n",
    "    return None\n",
    "\n",
    "# Apply the aggregation function for Hydraulic Conductivity and Coarse Fraction (case-insensitive)\n",
    "df_svsim['HydraulicConductivity'] = df_svsim['USCS'].apply(lambda x: aggregate_uscs_values(x, hydraulic_conductivity_mapping, agg_func='average'))\n",
    "df_svsim['AverageCoarseFraction'] = df_svsim['USCS'].apply(lambda x: aggregate_uscs_values(x, coarse_fraction_mapping, agg_func='average'))\n",
    "\n",
    "# Identify and display any unmatched values\n",
    "unmatched_values = df_svsim[df_svsim['HydraulicConductivity'].isna()]['USCS'].unique()\n",
    "print(\"Unmatched 'USCS' values:\", unmatched_values)\n",
    "\n",
    "# Clean data\n",
    "if 'Unnamed: 0' in df_svsim.columns:\n",
    "    df_svsim = df_svsim.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "# Convert the 'USCS' column to uppercase\n",
    "df_svsim['USCS'] = df_svsim['USCS'].str.upper()\n",
    "\n",
    "df_svsim.rename(columns={'SVSIM_NAME': 'WCRNUMBER'}, inplace=True)\n",
    "\n",
    "# Get the number of unique values in the 'WCRNUMBER' column\n",
    "unique_wcrnumber_count = df_svsim['WCRNUMBER'].nunique()\n",
    "print(f\"Number of unique WCRNUMBER values: {unique_wcrnumber_count}\")\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "df_svsim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file = '11-updated_SVSim.csv'\n",
    "df_svsim.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final updated CSV file saved as '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5aa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
